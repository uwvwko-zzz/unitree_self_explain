# æ£€æµ‹modelç»“æœï¼Ÿ

import os
import sys

# --- è·¯å¾„è®¾ç½® ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(script_dir, '../..'))
sys.path.insert(0, project_root)

import isaacgym
import torch
import numpy as np

from legged_gym.envs import *
from legged_gym.utils import get_args, task_registry
from legged_gym.utils.terrain import Terrain

# ========== Monkey Patch: ä¿®å¤ Terrain.selected_terrain ==========
def fixed_selected_terrain(self):
    from isaacgym import terrain_utils
    terrain_type = self.cfg.terrain_kwargs.pop('type')
    if not hasattr(terrain_utils, terrain_type):
        raise ValueError(f"Terrain type '{terrain_type}' not found in isaacgym.terrain_utils")
    terrain_func = getattr(terrain_utils, terrain_type)

    for k in range(self.cfg.num_sub_terrains):
        (i, j) = np.unravel_index(k, (self.cfg.num_rows, self.cfg.num_cols))
        terrain = terrain_utils.SubTerrain(
            "terrain",
            width=self.width_per_env_pixels,
            length=self.width_per_env_pixels,
            vertical_scale=self.cfg.vertical_scale,
            horizontal_scale=self.cfg.horizontal_scale,
        )
        terrain_func(terrain, **self.cfg.terrain_kwargs)
        self.add_terrain_to_map(terrain, i, j)

Terrain.selected_terrain = fixed_selected_terrain

def evaluate_model(env, policy, num_steps=1000, command=[1.0, 0.0, 0.0]):
    """
    å›ºå®šæŒ‡ä»¤ä¸‹è¿è¡Œæ¨¡å‹ï¼Œè¿”å›å¹³å‡ reward å’Œæœ€ç»ˆå‰è¿›è·ç¦»
    """
    env.reset()  # é‡ç½®ç¯å¢ƒï¼Œå†…éƒ¨ä¼šè®¾ç½® obs_buf, rew_buf ç­‰
    total_reward = 0.0

    with torch.no_grad():
        for _ in range(num_steps):
            # è®¾å®šå›ºå®šå‘½ä»¤
            env.commands[:, 0] = command[0]
            env.commands[:, 1] = command[1]
            env.commands[:, 2] = command[2]

            # è·å–è§‚æµ‹
            obs = env.obs_buf.clone()

            # æ¨ç†åŠ¨ä½œ
            actions = policy.actor(obs)

            # æ‰§è¡Œä¸€æ­¥ï¼ˆåªæ›´æ–°çŠ¶æ€ï¼Œä¸è¿”å› rewardï¼‰
            env.step(actions)

            # ä» rew_buf è·å– reward
            rewards = env.rew_buf.clone()  # shape: [num_envs]
            total_reward += rewards.sum().item()

    avg_reward = total_reward / num_steps
    forward_distance = env.base_pos[0, 0].item()  # å‡è®¾ç¬¬0ä¸ªç¯å¢ƒ

    return avg_reward, forward_distance

# ========== ä¸»è¯„ä¼°å‡½æ•° ==========
def evaluate_all_models(args, log_dir, command=[1.0, 0.0, 0.0], num_steps=1000):
    # è·å–é…ç½®
    env_cfg, train_cfg = task_registry.get_cfgs(name=args.task)
    env_cfg.env.num_envs = 1
    env_cfg.terrain.mesh_type = 'trimesh'
    env_cfg.terrain.curriculum = False
    env_cfg.terrain.selected = True
    env_cfg.terrain.terrain_kwargs = {
        "type": "pyramid_stairs_terrain",
        "step_width": 0.3,
        "step_height": 0.15,
        "platform_size": 2.0
    }
    env_cfg.terrain.num_rows = 1
    env_cfg.terrain.num_cols = 1
    env_cfg.commands.heading_command = False
    env_cfg.commands.resampling_time = 1e6
    env_cfg.env.episode_length_s = 1e6
    env_cfg.noise.add_noise = False
    env_cfg.domain_rand.push_robots = False
    env_cfg.env.test = True
    env_cfg.headless = True  # è‡ªåŠ¨è¯„ä¼°ï¼Œæ— éœ€ GUI

    # è·å–æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
    model_files = [f for f in os.listdir(log_dir) if f.startswith("model_") and f.endswith(".pt")]
    if not model_files:
        print(f"âŒ No model files found in {log_dir}")
        return

    # æå– step å¹¶æ’åº
    model_info = []
    for f in model_files:
        try:
            step = int(f.split('_')[1].split('.')[0])
            model_info.append((step, f))
        except:
            continue
    model_info.sort()

    # åˆ›å»ºç¯å¢ƒï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼Œå¤ç”¨ï¼‰
    env, _ = task_registry.make_env(name=args.task, args=args, env_cfg=env_cfg)

    print(f"ğŸ” Found {len(model_info)} models. Evaluating with command: {command}")

    results = []

    for step, filename in model_info:
        model_path = os.path.join(log_dir, filename)

        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        from rsl_rl.modules import ActorCritic
        policy = ActorCritic(
            num_actor_obs=env_cfg.env.num_observations,
            num_critic_obs=env_cfg.env.num_observations,
            num_actions=env_cfg.env.num_actions,
            actor_hidden_dims=train_cfg.policy.actor_hidden_dims,
            critic_hidden_dims=train_cfg.policy.critic_hidden_dims,
            activation=train_cfg.policy.activation,
            init_noise_std=getattr(train_cfg.policy, "init_noise_std", 1.0)
        ).to(env.device)

        # åŠ è½½æƒé‡
        ckpt = torch.load(model_path, map_location=env.device)
        if "model_state_dict" in ckpt:
            state_dict = ckpt["model_state_dict"]
            actor_dict = {k.replace("actor.", ""): v for k, v in state_dict.items() if k.startswith("actor.")}
            policy.actor.load_state_dict(actor_dict)
        else:
            policy.load_state_dict(ckpt)

        policy.eval()

        # è¯„ä¼°
        avg_reward, forward_dist = evaluate_model(env, policy, num_steps=num_steps, command=command)
        print(f"[Step {step:5d}] Avg Reward: {avg_reward:8.3f} | Forward: {forward_dist:6.2f} m")
        results.append((step, avg_reward, forward_dist, filename))

        # å¯é€‰ï¼šé‡ç½®ç¯å¢ƒç¡®ä¿å¹²å‡€çŠ¶æ€
        env.reset()

    # æ’åºï¼ˆæŒ‰ reward é™åºï¼‰
    results.sort(key=lambda x: x[1], reverse=True)

    print("\n" + "="*70)
    print("ğŸ† Top 5 Models (by Avg Reward)")
    print("="*70)
    for i, (step, r, dist, name) in enumerate(results[:5]):
        print(f"{i+1}. Step {step:5d} | {name:<15} | Reward: {r:8.3f} | Forward: {dist:6.2f} m")

    best = results[0]
    print(f"\nâœ… Best Model: {best[3]} (Step {best[0]}, Reward: {best[1]:.3f})")

    # ä¿å­˜ç»“æœ
    with open(os.path.join(log_dir, "auto_eval_results.txt"), "w") as f:
        f.write("Step,Filename,AvgReward,ForwardDistance\n")
        for step, r, dist, name in results:
            f.write(f"{step},{name},{r:.4f},{dist:.4f}\n")

    env.gym.destroy_sim(env.sim)

# ========== å…¥å£ ==========
if __name__ == '__main__':
    args = get_args()

    # è®¾ç½®ä½ çš„æ—¥å¿—ç›®å½•ï¼ˆåŒ…å«å¤šä¸ª model_XXXX.ptï¼‰
    log_dir = "/home/extra/zhy/æ¡Œé¢/IsaacGym_Preview_4_Package/isaacgym/unitree_rl_gym/logs/rough_go2/Nov24_12-53-28_"

    # è¯„ä¼°å‘½ä»¤ï¼šå‰è¿› 1.0 m/s
    evaluate_all_models(
        args,
        log_dir=log_dir,
        command=[1.0, 0.0, 0.0],
        num_steps=2000  # æ¯ä¸ªæ¨¡å‹è·‘ 2000 æ­¥
    )